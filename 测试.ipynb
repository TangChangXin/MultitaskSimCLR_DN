{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = torch.Tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "b = torch.Tensor([[2, 3, 4],\n",
    "                  [1, 4, 5]])\n",
    "l2规范化a, l2规范化b = F.normalize(a, dim=-1), F.normalize(b, dim=-1)\n",
    "print(\"l2规范化a\")\n",
    "print(l2规范化a)\n",
    "print(\"l2规范化b\")\n",
    "print(l2规范化b)\n",
    "print('\\n')\n",
    "拼接 = torch.cat([l2规范化a, l2规范化b], dim=0)\n",
    "# out.t().contiguous()矩阵转置后以行优先形式在内存中连续存储。\n",
    "# 计算余弦相似性，对角线表示每个图像自己和自己的相似性，需要去掉\n",
    "sim_matrix = torch.exp(torch.mm(拼接, 拼接.t().contiguous())) # 形状[2*batch_size, 2*batch_size]\n",
    "print(\"内积矩阵\")\n",
    "print(sim_matrix)\n",
    "print('\\n')\n",
    "# torch.ones_like()根据给定的张量生成全是1的张量，torch.eye()生成指定大小和类型的对角线全是1的2维张量。\n",
    "# mask是对角线全False，其余全True的张量\n",
    "mask = (torch.ones_like(sim_matrix) - torch.eye(2 * 2, device=sim_matrix.device)).bool()\n",
    "\n",
    "# 根据给定的掩码张量的二元值，取出输入张量中对应位置的值，这里就是去掉对角线的值，返回一个行优先排列的一维张量。\n",
    "# 然后改变形状，得到真正的余弦相似性矩阵，\n",
    "sim_matrix = sim_matrix.masked_select(mask).view(2 * 2, -1)# 形状[2*batch_size, 2*batch_size-1]\n",
    "print(\"相似矩阵\")\n",
    "print(sim_matrix)\n",
    "print('\\n')\n",
    "\n",
    "print(\"相似矩阵行求和\")\n",
    "print(sim_matrix.sum(dim=-1))\n",
    "print('\\n')\n",
    "\n",
    "print(\"规范化相乘再行求和\")\n",
    "pos_sim = torch.exp(torch.sum(l2规范化a * l2规范化b, dim=-1)) # 形状[1*batch_size]\n",
    "print(pos_sim)\n",
    "print('\\n')\n",
    "\n",
    "# [2*B]\n",
    "pos_sim = torch.cat([pos_sim, pos_sim], dim=0)\n",
    "print(\"拼接\")\n",
    "print(pos_sim)\n",
    "print('\\n')\n",
    "结果 = (- torch.log(pos_sim / sim_matrix.sum(dim=-1))).mean()\n",
    "print(结果)\n",
    "print(- torch.log(pos_sim / sim_matrix.sum(dim=-1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 2., 4., 5.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1, 2, 3])\n",
    "b = torch.Tensor([2, 4, 5])\n",
    "print(torch.cat((a, b),dim=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "A.shape:\n",
      " torch.Size([2, 3]) \n",
      "\n",
      "B:\n",
      " tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]]) \n",
      "B.shape:\n",
      " torch.Size([4, 3]) \n",
      "\n",
      "C:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]]) \n",
      "C.shape:\n",
      " torch.Size([6, 3]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "A=torch.ones(2,3)    #2x3的张量（矩阵）\n",
    "print(\"A:\\n\",A,\"\\nA.shape:\\n\",A.shape,\"\\n\")\n",
    "\n",
    "B=2*torch.ones(4,3)  #4x3的张量（矩阵）\n",
    "print(\"B:\\n\",B,\"\\nB.shape:\\n\",B.shape,\"\\n\")\n",
    "\n",
    "C=torch.cat((A,B),0)  #按维数0（行）拼接\n",
    "print(\"C:\\n\",C,\"\\nC.shape:\\n\",C.shape,\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 3., 2., 4., 3., 4.])\n",
      "tensor([ 5.,  8., 16.])\n",
      "tensor([ 5.,  8., 16.])\n",
      "tensor([ 5.,  8., 16.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[1, 1, 3],\n",
    "                  [2, 2, 4],\n",
    "                  [3, 4, 9]])\n",
    "掩码 = (torch.ones_like(a) - torch.eye(3)).bool()\n",
    "# print(掩码)\n",
    "print(torch.masked_select(a, 掩码))\n",
    "# print(掩码)\n",
    "print(a.sum(dim=-1))\n",
    "print(torch.sum(a, dim=-1))\n",
    "print(torch.sum(a, dim=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.6931, 1.0986])\n",
      "tensor(0.5973)\n",
      "tensor([1.0918])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1 ,2 ,3])\n",
    "print(torch.log(a))\n",
    "b = (torch.log(a)).mean()\n",
    "print(b)\n",
    "c = torch.Tensor([0.3356])\n",
    "print(-torch.log(c))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "[0 1 2 3 4]\n",
      "?\n",
      "[1 2 3 4 5]\n",
      "?\n",
      "[2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[[1, 2, 3, 4, 5],\n",
    "               [2 ,3 ,4 ,5 ,6],\n",
    "               [3, 4, 5, 6, 7]],\n",
    "\n",
    "              [[2, 3, 4, 5, 6],\n",
    "               [3, 4, 5, 6, 7],\n",
    "               [4, 5, 6, 7, 8]],\n",
    "\n",
    "              [[0, 1, 2, 3, 4],\n",
    "               [1, 2, 3, 4, 5],\n",
    "               [2, 2, 2, 2, 2]]])\n",
    "# b = a[-1, :, :]\n",
    "for i in a[-1, :, :]:\n",
    "    print(\"?\")\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 2\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3, 4, 5],[2 ,3 ,4 ,5 ,6],[3, 4, 5, 6, 7]])\n",
    "for i, (v1, v2, v3, v4, v5) in enumerate(a):\n",
    "    print(i, v1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "a = os.cpu_count()\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "asdfaaaaaaaaaaaa = np.array([1, 2, 3, 4])\n",
    "print(asdfaaaaaaaaaaaa)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}